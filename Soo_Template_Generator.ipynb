{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FhsbCaOIapRG"
      },
      "outputs": [],
      "source": [
        "from contextlib import closing\n",
        "import psycopg2\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "from tqdm import tqdm\n",
        "\n",
        "dataset_path = Path('./archieved').resolve() / 'dataset'\n",
        "datasets = dataset_path.glob('*.csv')\n",
        "\n",
        "DATASET_TABLE_MAP = {\n",
        "        \"noaa_tsunami\": \"historical_runups\",\n",
        "        \"thelook_ecommerce\": \"orders\",\n",
        "        \"sunroof_solar\": \"solar_potential_by_postal_code\",\n",
        "        \"sdoh_bea_cainc30\": \"fips\",\n",
        "        \"medicare\": \"outpatient_charges_2014\",\n",
        "        \"patents_dsep\": \"disclosures_13\",\n",
        "        \"ncaa_basketball\": \"mbb_historical_teams_games\"\n",
        "    }\n",
        "ENGINE_URL = \"postgresql+psycopg2://{db_user}@{db_url}/{db_name}\".format(\n",
        "    db_user=\"postgresml\",\n",
        "    db_url=\"localhost:25432\", # \"localhost:25432\", # \"147.47.236.50:25432\",\n",
        "    db_name=\"postgresml\"\n",
        ")\n",
        "PGML_SCHEMA = \"public\"\n",
        "CSV_DIR = Path(\"./archieved/dataset\")\n",
        "ENGINE = create_engine(ENGINE_URL, future=True)\n",
        "\n",
        "def insert_to_postgres():\n",
        "    for csv_path in CSV_DIR.glob(\"*.csv\"):\n",
        "        base = csv_path.stem.removesuffix(\"_cleaned\")\n",
        "        table = DATASET_TABLE_MAP.get(base, base)\n",
        "        with open(csv_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
        "            total = len(f.readlines())\n",
        "        first = True\n",
        "        for chunk in tqdm(pd.read_csv(csv_path, chunksize=100_000, encoding=\"utf-8-sig\"), total=total//100_000):\n",
        "            chunk.to_sql(\n",
        "                name=table,\n",
        "                con=ENGINE,\n",
        "                schema=PGML_SCHEMA,\n",
        "                if_exists=\"replace\" if first else \"append\",\n",
        "                index=False,\n",
        "                method=\"multi\"\n",
        "            )\n",
        "            first = False\n",
        "        print(f\"✓ {csv_path.name} -> {PGML_SCHEMA}.{table}\")\n",
        "\n",
        "# insert_to_postgres()\n",
        "\n",
        "from sqlalchemy import create_engine, text\n",
        "from sqlalchemy.exc import SQLAlchemyError\n",
        "\n",
        "def execute_query(query: str, params: dict = None, fetch: bool = False):\n",
        "    \"\"\"\n",
        "    Execute a raw SQL query with SQLAlchemy.\n",
        "    \n",
        "    Args:\n",
        "        query (str): The SQL query string (use :param for placeholders).\n",
        "        params (dict): Dictionary of parameters to bind.\n",
        "        fetch (bool): Whether to fetch results (for SELECT queries).\n",
        "    \n",
        "    Returns:\n",
        "        list of dicts if fetch=True, else number of affected rows.\n",
        "\n",
        "    Usage:\n",
        "    ```\n",
        "    # SELECT query\n",
        "    rows = execute_query(\"SELECT * FROM users WHERE age > :age\", {\"age\": 21}, fetch=True)\n",
        "    print(rows)\n",
        "\n",
        "    # INSERT query\n",
        "    inserted = execute_query(\n",
        "        \"INSERT INTO users (name, age) VALUES (:name, :age)\",\n",
        "        {\"name\": \"Alice\", \"age\": 25}\n",
        "    )\n",
        "    print(f\"Inserted rows: {inserted}\")\n",
        "\n",
        "    # UPDATE query\n",
        "    updated = execute_query(\n",
        "        \"UPDATE users SET age = :age WHERE name = :name\",\n",
        "        {\"name\": \"Alice\", \"age\": 26}\n",
        "    )\n",
        "    print(f\"Updated rows: {updated}\")\n",
        "    ```\n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        with ENGINE.connect() as connection:\n",
        "            result = connection.execute(text(query), params or {})\n",
        "            \n",
        "            if fetch:\n",
        "                # Fetch results as list of dicts\n",
        "                rows = [dict(row._mapping) for row in result]\n",
        "                return rows\n",
        "            else:\n",
        "                # Commit changes for write queries\n",
        "                connection.commit()\n",
        "                return result.rowcount\n",
        "    except SQLAlchemyError as e:\n",
        "        print(f\"Database error: {e}\")\n",
        "        return None\n",
        "    \n",
        "\n",
        "def create_random_split(table_name: str, split_criteria: float=0.9, train_view_name=\"train\", test_view_name=\"test\"):\n",
        "    \"\"\"\n",
        "    Create a stable random train/test split by materializing a random number into a new table.\n",
        "    Works without a primary key.\n",
        "    \"\"\"\n",
        "    engine = create_engine(ENGINE_URL, future=True)\n",
        "    table_with_rnd = f\"{PGML_SCHEMA}.{table_name}_with_rnd\"\n",
        "    with engine.begin() as conn:\n",
        "        # Drop old objects if they exist\n",
        "        conn.execute(text(f\"DROP VIEW IF EXISTS {train_view_name} CASCADE\"))\n",
        "        conn.execute(text(f\"DROP VIEW IF EXISTS {test_view_name} CASCADE\"))\n",
        "        conn.execute(text(f\"DROP TABLE IF EXISTS {table_with_rnd} CASCADE\"))\n",
        "        # Get original column Names\n",
        "        original_columns = conn.execute(text(f\"SELECT column_name FROM information_schema.columns WHERE table_name = '{table_name}'\")).fetchall()\n",
        "        original_columns = [col[0] for col in original_columns]\n",
        "        col_list = \", \".join(original_columns)\n",
        "        # Create a split table with a random assignment column\n",
        "        conn.execute(text(f\"\"\"\n",
        "            CREATE TABLE {table_with_rnd} AS\n",
        "            SELECT *, RANDOM() AS rnd\n",
        "            FROM {table_name};\n",
        "        \"\"\"))\n",
        "\n",
        "        # Train view\n",
        "        conn.execute(text(f\"\"\"\n",
        "            CREATE VIEW {table_name}_{train_view_name} AS\n",
        "            SELECT {col_list} \n",
        "            FROM {table_with_rnd}\n",
        "            WHERE rnd < {split_criteria};\n",
        "        \"\"\"))\n",
        "\n",
        "        # Test view\n",
        "        conn.execute(text(f\"\"\"\n",
        "            CREATE VIEW {test_view_name} AS\n",
        "            SELECT {col_list} \n",
        "            FROM {table_with_rnd}\n",
        "            WHERE rnd >= {split_criteria};\n",
        "        \"\"\"))\n",
        "\n",
        "    print(f\"Stable random split created: '{train_view_name}' and '{test_view_name}'\")\n",
        "\n",
        "def drop_views(table_name: str):\n",
        "    execute_query(f\"DROP VIEW IF EXISTS {table_name}_train CASCADE\")\n",
        "    execute_query(f\"DROP VIEW IF EXISTS {table_name}_test CASCADE\")\n",
        "    print(f\"Dropped views for table: {table_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TODO \n",
        "# 1. train/test 셋 생성 o\n",
        "# 2. 모델 훈련\n",
        "# 3. test 데이터 퍼포먼스 저장\n",
        "# 4. Skyline 확인\n",
        "\n",
        "table_specific_exclusions = {\n",
        "    \"orders\": [], #done\n",
        "    \"disclosures_13\": [\"wg_name\"], #done\n",
        "    \"outpatient_charges_2014\": [], #done, no missing values\n",
        "    \"fips\": [\"Proprietors_income\", \"Nonfarm_proprietors_income\", \"Net_earnings_by_place_of_residence\"], #done\n",
        "    \"mbb_historical_teams_games\": [], #done\n",
        "    \"historical_runups\": [\"id\", \"timestamp\", \"location_name\"],\n",
        "    \"solar_potential_by_postal_code\": [],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'tsevent_id, year, month, day, doubtful, country, state, latitude, longitude, region_code, distance_from_source, arr_day, arr_hour, arr_min, travel_time_hours, travel_time_minutes, water_ht, horizontal_inundation, type_measurement_id, period, first_motion, deaths, deaths_description, injuries, injuries_description, damage_millions_dollars, damage_description, houses_damaged, houses_damaged_description, houses_destroyed, houses_destroyed_description'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "table_name = \"historical_runups\"\n",
        "query = f\"\"\"SELECT * FROM public.{table_name};\"\"\"\n",
        "x = execute_query(query, fetch=True)\n",
        "df = pd.DataFrame(x)\n",
        "cols_list = \", \".join([c for c in df.columns.tolist() if c not in table_specific_exclusions.get(table_name, [])])\n",
        "cols_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stable random split created: 'train' and 'test'\n",
            "Dropped views for table: historical_runups\n"
          ]
        }
      ],
      "source": [
        "create_random_split(\"historical_runups\")\n",
        "drop_views(\"historical_runups\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CREATE VIEW public.historical_runups_view AS (\n",
            "SELECT tsevent_id, year, month, day, doubtful, country, state, latitude, longitude, region_code, distance_from_source, arr_day, arr_hour, arr_min, travel_time_hours, travel_time_minutes, water_ht, horizontal_inundation, type_measurement_id, period, first_motion, deaths, deaths_description, injuries, injuries_description, damage_millions_dollars, damage_description, houses_damaged, houses_damaged_description, houses_destroyed, houses_destroyed_description\n",
            "FROM public.historical_runups\n",
            ");\n"
          ]
        }
      ],
      "source": [
        "relation_name = f\"{PGML_SCHEMA}.{table_name}_view\"\n",
        "query = f\"\"\"CREATE VIEW {relation_name} AS (\n",
        "SELECT {cols_list}\n",
        "FROM {PGML_SCHEMA}.{table_name}\n",
        ");\"\"\"\n",
        "\n",
        "print(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SELECT pgml.train(\n",
            "    project_name => 'noaa_tsunami_public.historical_runups',\n",
            "    task => 'regression',\n",
            "    relation_name => 'public.historical_runups_view',\n",
            "    y_column_name => 'period',\n",
            "    algorithm => 'xgboost',\n",
            "    preprocess => '{\"country\": {\"impute\": \"mode\"}}',\n",
            "    test_size => 0.2\n",
            ");\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "def build_pgml_train_sql(project_name: str, task: str, relation_name: str, y_column_name: str, algorithm: str, preprocess: dict | None = None, test_size: float | None = None):\n",
        "    \"\"\"\n",
        "    \n",
        "    preprocess: \n",
        "        impute: `error`, `mean`, `median`, `mode`, `min`, `max`, `zero`\n",
        "        scale: `preserve`, `standard`, `min_max`, `max_abs`, `robust`\n",
        "        encode: `native`, `target`, `one_hot`, `ordinal`\n",
        "    \"\"\"\n",
        "    args = [\n",
        "        f\"project_name => '{project_name}'\",\n",
        "        f\"task => '{task}'\",\n",
        "        f\"relation_name => '{relation_name}'\",\n",
        "        f\"y_column_name => '{y_column_name}'\",\n",
        "        f\"algorithm => '{algorithm}'\",\n",
        "    ]\n",
        "    if preprocess:\n",
        "        args.append(f\"preprocess => '{json.dumps(preprocess, ensure_ascii=False)}'\")\n",
        "    if test_size is not None:\n",
        "        args.append(f\"test_size => {test_size}\")\n",
        "    return \"SELECT pgml.train(\\n    \" + \",\\n    \".join(args) + \"\\n);\\n\"\n",
        "\n",
        "model_families = {\n",
        "    \"regression\": ['xgboost', 'ada_boost', 'random_forest', 'gradient_boosting_trees', 'bagging', 'svm', 'ridge', 'bayesian_ridge'],\n",
        "    \"classification\": ['xgboost', 'ada_boost', 'random_forest', 'gradient_boosting_trees', 'bagging', 'svm', 'ridge'],\n",
        "    \"clustering\": ['kmeans', 'mini_batch_kmeans'],\n",
        "}\n",
        "\n",
        "query = build_pgml_train_sql(\n",
        "    project_name=f\"noaa_tsunami_{PGML_SCHEMA}.historical_runups\",\n",
        "    task=\"regression\",\n",
        "    relation_name=relation_name,\n",
        "    y_column_name=\"period\",\n",
        "    algorithm=\"xgboost\",\n",
        "    preprocess={\"country\": {\"impute\": \"mode\"}},\n",
        "    test_size=0.2,\n",
        "    \n",
        ")\n",
        "\n",
        "print(query)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Database error: (psycopg2.errors.InternalError_) 0 missing values for country. You may provide a preprocessor to impute a value. e.g:\n",
            "\n",
            " pgml.train(preprocessor => '{\"country\": {\"impute\": \"mean\"}}'\n",
            "\n",
            "[SQL: SELECT pgml.train(\n",
            "    project_name => 'noaa_tsunami_public.historical_runups',\n",
            "    task => 'regression',\n",
            "    relation_name => 'public.historical_runups_view',\n",
            "    y_column_name => 'period',\n",
            "    algorithm => 'xgboost',\n",
            "    test_size => 0.2\n",
            ");\n",
            "]\n",
            "(Background on this error at: https://sqlalche.me/e/20/2j85)\n"
          ]
        }
      ],
      "source": [
        "execute_query(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total unique intent: 225 | S1: 131 | S2: 94\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import psycopg2\n",
        "from google.cloud import bigquery\n",
        "from dotenv import load_dotenv\n",
        "import logging\n",
        "\n",
        "# Configure basic logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Load environment variables from a .env file\n",
        "load_dotenv()\n",
        "PROJECT_ID = os.getenv(\"PROJECT_ID\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xjzfx1_btRv"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import hashlib\n",
        "import random\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "jsonl_path = \"/content/data_for_SQL_generation.json\" # File with only selected datasets\n",
        "output_path = \"/content/SQLs.jsonl\"\n",
        "PROJECT_ID = \"crypto-isotope-366706\"\n",
        "DATA_ID = \"bigquery-public-data\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD340QHIjRBk"
      },
      "source": [
        "BigqueryML models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoqijHmbbprV"
      },
      "outputs": [],
      "source": [
        "model_families = {\n",
        "    \"Time Series\": {\n",
        "        \"regression\": [\"ARIMA_PLUS\", \"ARIMA_PLUS_XREG\", \"LINEAR_REG\", \"BOOSTED_TREE_REGRESSOR\", \"DNN_REGRESSOR\", \"DNN_LINEAR_COMBINED_REGRESSOR\" \"RANDOM_FOREST_REGRESSOR\"],\n",
        "        \"classification\": [\"LOGISTIC_REG\", \"BOOSTED_TREE_CLASSIFIER\", \"DNN_CLASSIFIER\", \"DNN_LINEAR_COMBINED_CLASSIFIER\", \"RANDOM_FOREST_CLASSIFIER\"],\n",
        "        \"clustering\": [\"KMEANS\"],\n",
        "        \"anomaly_detection\": [\"KMEANS\", \"ARIMA_PLUS\", \"ARIMA_PLUS_XREG\"]\n",
        "\n",
        "    },\n",
        "    \"Non Time Series\":{\n",
        "        \"regression\": [\"LINEAR_REG\", \"BOOSTED_TREE_REGRESSOR\", \"DNN_REGRESSOR\", \"DNN_LINEAR_COMBINED_REGRESSOR\", \"RANDOM_FOREST_REGRESSOR\"],\n",
        "        \"classification\": [\"LOGISTIC_REG\", \"BOOSTED_TREE_CLASSIFIER\", \"DNN_CLASSIFIER\", \"DNN_LINEAR_COMBINED_CLASSIFIER\", \"RANDOM_FOREST_CLASSIFIER\"],\n",
        "        \"clustering\": [\"KMEANS\"],\n",
        "        \"anomaly_detection\": [\"KMEANS\"]\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a2xax0vjVFW"
      },
      "source": [
        "PostgreML models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4IKYdPG99cPw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import uuid\n",
        "\n",
        "PGML_SCHEMA = os.getenv(\"PGML_SCHEMA\", \"public\")\n",
        "PGML_ALLOWED = {\n",
        "    \"regression\": ['xgboost', 'ada_boost', 'random_forest', 'gradient_boosting_trees', 'bagging', 'svm', 'ridge', 'bayesian_ridge'],\n",
        "    \"classification\": ['xgboost', 'ada_boost', 'random_forest', 'gradient_boosting_trees', 'bagging', 'svm', 'ridge'],\n",
        "    \"clustering\": ['kmeans', 'mini_batch_kmeans'],\n",
        "}\n",
        "PGML_DEFAULT = {\n",
        "    \"regression\": \"xgboost\",\n",
        "    \"classification\": \"xgboost\",\n",
        "    \"clustering\": \"kmeans\",\n",
        "}\n",
        "COMPARISON_OPS = {\">\", \"<\", \">=\", \"<=\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPCQ9IAdjYxK"
      },
      "source": [
        "Excluded columns (Bigquery) and missing values (for pgml)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Literal, Optional, Any\n",
        "import psycopg2\n",
        "from psycopg2 import sql\n",
        "from psycopg2.extras import execute_values\n",
        "\n",
        "class TemplateGenerator():\n",
        "    comparison_ops = {\">\", \"<\", \">=\", \"<=\"}\n",
        "\n",
        "    def __init__(self, platform_type: Literal[\"bigquery\", \"postgresql\"]):\n",
        "        self.platform_type = platform_type\n",
        "\n",
        "    def extract_tag_value(self, tagged_str: str, tag: str) -> str:\n",
        "        # Extracts <tag>value</tag> from a string\n",
        "        if not tagged_str:\n",
        "            return \"\"\n",
        "        import re\n",
        "        pattern = f\"<{tag}>(.*?)</{tag}>\"\n",
        "        match = re.search(pattern, tagged_str)\n",
        "        return match.group(1) if match else \"\"\n",
        "    \n",
        "    def format_val(self, val: str) -> str:\n",
        "        try:\n",
        "            float(val)\n",
        "            return val\n",
        "        except ValueError:\n",
        "            return f\"'{val}'\"\n",
        "\n",
        "    def parse_cond(self, cond: str) -> tuple[str, str, str]:\n",
        "        col = self.extract_tag_value(cond, \"col\")\n",
        "        op = self.extract_tag_value(cond, \"op\")\n",
        "        val = self.extract_tag_value(cond, \"val\")\n",
        "        return col, op, val\n",
        "    \n",
        "    def split_update_conditions(self, update_conds_raw: list[str]) -> tuple[list[str], list[str]]:\n",
        "        filter_like, true_updates = [], []\n",
        "        for cond in update_conds_raw or []:\n",
        "            col, op, val = self.parse_cond(cond)\n",
        "            if op in self.comparison_ops:\n",
        "                filter_like.append(cond)\n",
        "            else:\n",
        "                true_updates.append(cond)\n",
        "        return filter_like, true_updates\n",
        "    \n",
        "    def quote_ident(self, name: str) -> str:\n",
        "        quote_char = '\"' if self.platform_type == \"postgresql\" else \"`\"\n",
        "        if name is None:\n",
        "            return ''\n",
        "        s = str(name).strip()\n",
        "        if s.startswith(quote_char) and s.endswith(quote_char):\n",
        "            return s\n",
        "        s = s.replace(quote_char, f\"{quote_char}{quote_char}\")\n",
        "        return f\"{quote_char}{s}{quote_char}\"\n",
        "    \n",
        "    def get_input_feature_columns_from_schema(self, schema: dict[str, dict[str, dict[str, Any]]], \n",
        "                                              exclude_cols: Optional[list[str]|None]) -> list[str]:\n",
        "        \"\"\"\n",
        "        {\n",
        "            <table_name>: {\n",
        "                'columns': {\n",
        "                    <column_name1>: dict,\n",
        "                    <column_name2>: dict,\n",
        "                    ...\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        \"\"\"\n",
        "        cols = []\n",
        "        for _, columns in schema.items():\n",
        "            for col in columns['columns'].keys():\n",
        "                if exclude_cols is None or col not in exclude_cols:\n",
        "                    cols.append(col)\n",
        "        return cols\n",
        "    \n",
        "    def gen(self, dataset_name: str, table_name: str, schema: dict, intent: dict, is_train: bool, model_name: str, **kwargs) -> dict:\n",
        "        \"\"\"\n",
        "        intent: the intend dictionary with five keys - `time_series`, `target_column`, `inference_condition`, `update_condition`, `task`\n",
        "        model_name: the name of the model to be used\n",
        "        schema: data_dictionary with {table_name: {'columns': {column_name: column_infos[dict], ... }}}\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"This method should be implemented by subclasses.\")\n",
        "\n",
        "class BigQueryTemplateGenerator(TemplateGenerator):\n",
        "    model_families = {\n",
        "        \"Time Series\": {\n",
        "            \"regression\": [\"ARIMA_PLUS\", \"ARIMA_PLUS_XREG\", \"LINEAR_REG\", \"BOOSTED_TREE_REGRESSOR\", \"DNN_REGRESSOR\", \"DNN_LINEAR_COMBINED_REGRESSOR\" \"RANDOM_FOREST_REGRESSOR\"],\n",
        "            \"classification\": [\"LOGISTIC_REG\", \"BOOSTED_TREE_CLASSIFIER\", \"DNN_CLASSIFIER\", \"DNN_LINEAR_COMBINED_CLASSIFIER\", \"RANDOM_FOREST_CLASSIFIER\"],\n",
        "            \"clustering\": [\"KMEANS\"],\n",
        "            \"anomaly_detection\": [\"KMEANS\", \"ARIMA_PLUS\", \"ARIMA_PLUS_XREG\"]\n",
        "\n",
        "        },\n",
        "        \"Non Time Series\":{\n",
        "            \"regression\": [\"LINEAR_REG\", \"BOOSTED_TREE_REGRESSOR\", \"DNN_REGRESSOR\", \"DNN_LINEAR_COMBINED_REGRESSOR\", \"RANDOM_FOREST_REGRESSOR\"],\n",
        "            \"classification\": [\"LOGISTIC_REG\", \"BOOSTED_TREE_CLASSIFIER\", \"DNN_CLASSIFIER\", \"DNN_LINEAR_COMBINED_CLASSIFIER\", \"RANDOM_FOREST_CLASSIFIER\"],\n",
        "            \"clustering\": [\"KMEANS\"],\n",
        "            \"anomaly_detection\": [\"KMEANS\"]\n",
        "        }\n",
        "    }\n",
        "    def __init__(self, platform_type):\n",
        "        super().__init__(platform_type)\n",
        "\n",
        "    def generate_hash(self, name: str) -> str:\n",
        "        return hashlib.md5(name.encode()).hexdigest()[:6]\n",
        "    \n",
        "    def gen(self, dataset_name: str, table_name: str, schema: dict, intent: dict, is_train: bool, model_name: str) -> str:\n",
        "        # dataset_name = intent[\"dataset_name\"]\n",
        "        # table_name = intent[\"table_id\"]\n",
        "        full_table = f\"`{DATA_ID}.{dataset_name}.{table_name}`\"\n",
        "\n",
        "        output = intent[\"output\"]\n",
        "        task = output[\"task\"]\n",
        "        is_time_series = output.get(\"time_series\", \"False\") == \"True\"\n",
        "        target_col = self.extract_tag_value(output.get(\"target_column\", \"\"), \"col\")\n",
        "        inference_conds_raw = output.get(\"inference_condition\", [])\n",
        "        update_conds_raw = output.get(\"update_condition\", [])\n",
        "\n",
        "        model_prefix = f\"crypto-isotope-366706.sql_knowledge_base.{target_col or task}\"\n",
        "        base_model_name = f\"{model_prefix}_{model_name.lower().replace('-', '_')}\"\n",
        "        full_model_name = f\"{base_model_name}_{self.generate_hash(base_model_name)}\"\n",
        "\n",
        "        # Separate update conditions\n",
        "        filter_like_updates, true_updates = [], []\n",
        "        for cond in update_conds_raw:\n",
        "            (filter_like_updates if self.extract_tag_value(cond, \"op\") in COMPARISON_OPS else true_updates).append(cond)\n",
        "\n",
        "        updated_cols = {self.extract_tag_value(cond, \"col\") for cond in true_updates}\n",
        "        filtered_inference_conds = [cond for cond in inference_conds_raw if self.extract_tag_value(cond, \"col\") not in updated_cols]\n",
        "        all_filters = filtered_inference_conds + filter_like_updates\n",
        "\n",
        "        where_clause = \" AND \".join(\n",
        "            f\"{self.quote_ident(self.extract_tag_value(c, 'col'))} {self.extract_tag_value(c, 'op')} {self.format_val(self.extract_tag_value(c, 'val'))}\"\n",
        "            for c in all_filters\n",
        "        )\n",
        "\n",
        "        needs_label = target_col and task not in [\"clustering\", \"anomaly_detection\"]\n",
        "        label_opt = f\", INPUT_LABEL_COLS=['{target_col}']\" if needs_label else \"\"\n",
        "        where_filter = f\"WHERE {target_col} IS NOT NULL\" if needs_label else \"\"\n",
        "\n",
        "        # Identify time column\n",
        "        data_dict = intent.get(\"data_dictionary\", {})\n",
        "        timestamp_cols_by_priority = {\"TIMESTAMP\": [], \"DATETIME\": [], \"DATE\": []}\n",
        "        for col, meta in data_dict.items():\n",
        "            col_type = meta.get(\"type\")\n",
        "            if col_type in timestamp_cols_by_priority:\n",
        "                timestamp_cols_by_priority[col_type].append(col)\n",
        "\n",
        "        time_col = None\n",
        "        if is_time_series:\n",
        "            for t in [\"TIMESTAMP\", \"DATETIME\", \"DATE\"]:\n",
        "                if timestamp_cols_by_priority[t]:\n",
        "                    time_col = timestamp_cols_by_priority[t][0]\n",
        "                    break\n",
        "            if not time_col:\n",
        "                raise ValueError(\"No time column found for time-series model.\")\n",
        "            if not target_col:\n",
        "                raise ValueError(\"No target_col provided for time-series model.\")\n",
        "\n",
        "        # Excluded columns\n",
        "        timestamp_cols = {col for col, meta in data_dict.items() if meta.get(\"type\") in {\"TIMESTAMP\", \"DATE\", \"DATETIME\"}}\n",
        "        excluded_cols = timestamp_cols\n",
        "        if is_time_series:\n",
        "            excluded_cols.update({time_col, target_col})\n",
        "\n",
        "        excluded_cols.update(table_specific_exclusions.get(table_name, []))\n",
        "        excluded_cols = {c for c in excluded_cols if c}\n",
        "\n",
        "        exclude_clause = \", \".join(sorted(excluded_cols))\n",
        "        select_clause = f\"* EXCEPT({exclude_clause})\" if excluded_cols else \"*\"\n",
        "\n",
        "        # Subquery for inference\n",
        "        subquery = f\"(SELECT * FROM {full_table}\"\n",
        "        if where_clause:\n",
        "            subquery += f\" WHERE {where_clause}\"\n",
        "        subquery += \")\"\n",
        "\n",
        "        # Training SQL\n",
        "        if is_train:\n",
        "            if is_time_series:\n",
        "                ts_opts = [\n",
        "                    f\"TIME_SERIES_TIMESTAMP_COL='{time_col}'\",\n",
        "                    f\"TIME_SERIES_DATA_COL='{target_col}'\"\n",
        "                ]\n",
        "                if \"xreg\" in model_name.lower():\n",
        "                    training_sql = f\"\"\"\n",
        "                    CREATE MODEL IF NOT EXISTS `{full_model_name}`\n",
        "                    OPTIONS(model_type='ARIMA_PLUS_XREG', {\", \".join(ts_opts)})\n",
        "                    AS\n",
        "                    (SELECT {time_col}, {target_col}, {select_clause} FROM {full_table}\n",
        "                    {where_filter})\n",
        "                    \"\"\".strip()\n",
        "                else:\n",
        "                    training_sql = f\"\"\"\n",
        "                    CREATE MODEL IF NOT EXISTS `{full_model_name}`\n",
        "                    OPTIONS(model_type='ARIMA_PLUS', {\", \".join(ts_opts)})\n",
        "                    AS\n",
        "                    (SELECT {time_col}, {target_col} FROM {full_table}\n",
        "                    {where_filter})\n",
        "                    \"\"\".strip()\n",
        "            else:\n",
        "                training_sql = f\"\"\"\n",
        "                CREATE MODEL IF NOT EXISTS `{full_model_name}`\n",
        "                OPTIONS(model_type='{model_name}'{label_opt},\n",
        "                DATA_SPLIT_METHOD='RANDOM',\n",
        "                DATA_SPLIT_EVAL_FRACTION=0.10)\n",
        "                AS\n",
        "                (SELECT {select_clause} FROM {full_table}\n",
        "                {where_filter})\n",
        "                \"\"\".strip()\n",
        "            return training_sql\n",
        "        # Inference SQL\n",
        "        else:\n",
        "            if task == \"anomaly_detection\":\n",
        "                inference_sql = f\"\"\"\n",
        "                SELECT * FROM ML.DETECT_ANOMALIES(\n",
        "                  MODEL `{full_model_name}`,\n",
        "                  STRUCT(0.2 AS contamination),\n",
        "                  {subquery}\n",
        "                )\n",
        "                \"\"\".strip()\n",
        "            elif is_time_series:\n",
        "                if \"xreg\" in model_name.lower():\n",
        "                    inference_sql = f\"\"\"\n",
        "                    SELECT * FROM ML.FORECAST(\n",
        "                      MODEL `{full_model_name}`,\n",
        "                      STRUCT(10 AS horizon, 0.8 AS confidence_level),\n",
        "                      {subquery}\n",
        "                    )\n",
        "                    \"\"\".strip()\n",
        "                elif \"arima_plus\" in model_name.lower():\n",
        "                    inference_sql = f\"\"\"\n",
        "                    SELECT * FROM ML.FORECAST(\n",
        "                      MODEL `{full_model_name}`,\n",
        "                      STRUCT(10 AS horizon, 0.8 AS confidence_level))\n",
        "                    \"\"\".strip()\n",
        "                else:\n",
        "                    inference_sql = f\"\"\"\n",
        "                    SELECT * FROM ML.PREDICT(MODEL `{full_model_name}`, {subquery})\n",
        "                    \"\"\".strip()\n",
        "            else:\n",
        "                inference_sql = f\"\"\"\n",
        "                SELECT * FROM ML.PREDICT(MODEL `{full_model_name}`, {subquery})\n",
        "                \"\"\".strip()\n",
        "            return inference_sql\n",
        "\n",
        "def _merge_user(custom_for_col: dict, base: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Deep-ish merge: only keys present in `custom_for_col` override.\n",
        "    Unknown keys are ignored (keeps output clean for downstream systems).\n",
        "    \"\"\"\n",
        "    out = base.copy()\n",
        "    for k in (\"impute\", \"scale\", \"encode\"):\n",
        "        if k in custom_for_col:\n",
        "            out[k] = custom_for_col[k]\n",
        "    return out\n",
        "\n",
        "def _flatten_schema(schema: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Accepts either:\n",
        "    {<table>: {\"columns\": {<col>: {\"type\": ...}}}}\n",
        "    or directly:\n",
        "    {\"columns\": {...}}\n",
        "    Returns {column_name: type_string}\n",
        "    \"\"\"\n",
        "    # If schema already at columns level\n",
        "    if \"columns\" in schema:\n",
        "        return {c: (v.get(\"type\") or \"\").upper() for c, v in schema[\"columns\"].items()}\n",
        "\n",
        "    # Else assume top-level tables\n",
        "    out = {}\n",
        "    for _tbl, spec in schema.items():\n",
        "        cols = spec.get(\"columns\", {})\n",
        "        out.update({c: (v.get(\"type\") or \"\").upper() for c, v in cols.items()})\n",
        "    return out\n",
        "\n",
        "\n",
        "class PostgresTemplateGenerator(TemplateGenerator):\n",
        "    pgml_schema = os.getenv(\"PGML_SCHEMA\", \"public\")\n",
        "    model_families = {\n",
        "        \"regression\": ['xgboost', 'ada_boost', 'random_forest', 'gradient_boosting_trees', 'bagging', 'svm', 'ridge', 'bayesian_ridge'],\n",
        "        \"classification\": ['xgboost', 'ada_boost', 'random_forest', 'gradient_boosting_trees', 'bagging', 'svm', 'ridge'],\n",
        "        \"clustering\": ['mini_batch_kmeans', 'affinity_propagation'],\n",
        "    }\n",
        "    # pgml_default = {\n",
        "    #     \"regression\": \"xgboost\",\n",
        "    #     \"classification\": \"xgboost\",\n",
        "    #     \"clustering\": \"kmeans\",\n",
        "    # }\n",
        "    def __init__(self):\n",
        "        super().__init__(platform_type=\"postgresql\")\n",
        "\n",
        "    def split_to_views(self, table_name: str):\n",
        "        create_random_split(f\"{self.pgml_schema}.{table_name}\")\n",
        "        \n",
        "    def drop_views(self, table_name: str):\n",
        "        drop_views(f\"{self.pgml_schema}.{table_name}\")\n",
        "\n",
        "    def _gen_preprocess(self, schema: dict, custom: Optional[dict|None]=None) -> dict:\n",
        "        \"\"\"\n",
        "        custom: {column: {impute|scale|encode: ...}}\n",
        "        Valid options:\n",
        "            impute:  error | mean | median | mode | min | max | zero\n",
        "            scale:   preserve | standard | min_max | max_abs | robust\n",
        "            encode:  native | target | one_hot | ordinal\n",
        "        Returns:\n",
        "            {column: {impute, scale, encode}}\n",
        "        \"\"\"\n",
        "        defaults_by_type = {\n",
        "            \"STRING\":   {\"impute\": \"mode\", \"encode\": \"native\"},\n",
        "            \"FLOAT\":    {\"impute\": \"median\", \"scale\": \"standard\"},\n",
        "            \"INTEGER\":  {\"impute\": \"median\", \"scale\": \"standard\"},\n",
        "            \"GEOGRAPHY\":{\"impute\": \"mode\", \"encode\": \"native\"},\n",
        "        }\n",
        "        \n",
        "        custom = custom or {}\n",
        "        col_types = _flatten_schema(schema)\n",
        "        result = {}\n",
        "\n",
        "        for col, typ in col_types.items():\n",
        "            base_type = defaults_by_type.get(typ, {\"impute\": \"median\", \"scale\": \"standard\", \"encode\": \"native\"})\n",
        "            # Merge any user overrides\n",
        "            if col in custom:\n",
        "                base_type = _merge_user(custom[col], base_type)\n",
        "            result[col] = base_type\n",
        "\n",
        "        return result\n",
        "    \n",
        "    def gen(self, dataset_name: str, table_name: str, schema: dict, intent: dict, is_train: bool, model_name: str, \n",
        "            exclude_cols: Optional[list[str]]=[], test_size: Optional[float|None]=0.1, **kwargs) -> dict:\n",
        "        \"\"\"\n",
        "        Generate the SQL queries and view definitions for the given parameters.\n",
        "        \n",
        "        \n",
        "        ```\n",
        "        train_output = generator.gen(\n",
        "            dataset_name=x['dataset_name'],\n",
        "            table_name=x['table_name'],\n",
        "            schema=x['schema'],\n",
        "            intent=x['intent'],\n",
        "            is_train=True,\n",
        "            model_name=x['model_name'],\n",
        "            exclude_cols=table_specific_exclusions.get(x['table_name'], [])\n",
        "        )\n",
        "        inference_output = generator.gen(\n",
        "            dataset_name=x['dataset_name'],\n",
        "            table_name=x['table_name'],\n",
        "            schema=x['schema'],\n",
        "            intent=x['intent'],\n",
        "            is_train=False,\n",
        "            model_name=x['model_name'],\n",
        "            exclude_cols=table_specific_exclusions.get(x['table_name'], [])\n",
        "        )\n",
        "        ```\n",
        "        \n",
        "        \"\"\"\n",
        "        # assert the model_name is the algorithm name\n",
        "        assert model_name in self.model_families.get(intent.get(\"task\"), []), f\"Model {model_name} is not suitable for task {intent.get('task')}\"\n",
        "\n",
        "        task = (intent.get(\"task\") or \"\").lower()\n",
        "        is_time_series = intent.get(\"time_series\", \"False\") == \"True\"\n",
        "        target_col = self.extract_tag_value(intent.get(\"target_column\", \"\"), \"col\")\n",
        "\n",
        "        # Generate unique model/project name\n",
        "        project_name = f\"{dataset_name}/{table_name}/{task}/{model_name}\"\n",
        "\n",
        "        input_feature_cols = self.get_input_feature_columns_from_schema(schema, [target_col]+exclude_cols)\n",
        "\n",
        "        # === TRAINING ===\n",
        "        if is_train:\n",
        "            # Parse input features\n",
        "            relation_name = f\"{self.pgml_schema}.{table_name}_view\"\n",
        "            # Create a view table for training.\n",
        "            tuple_expr = \", \".join(self.quote_ident(c) for c in input_feature_cols)\n",
        "            target_expr = ', ' + self.quote_ident(target_col) if target_col else ''\n",
        "            view_table_query = f\"\"\"CREATE OR REPLACE VIEW {relation_name} AS (\\nSELECT {tuple_expr}{target_expr} \\nFROM {self.pgml_schema}.{table_name});\"\"\"\n",
        "            \n",
        "            # Preprocess arguments\n",
        "            preprocess = self._gen_preprocess(schema, **kwargs)\n",
        "            if task in (\"regression\", \"classification\", \"clustering\"):\n",
        "                query = self.build_pgml_train_sql(\n",
        "                    project_name=project_name,\n",
        "                    task=task,\n",
        "                    relation_name=relation_name,\n",
        "                    y_column_name=target_col,\n",
        "                    algorithm=model_name,\n",
        "                    preprocess=preprocess,\n",
        "                    test_size=test_size\n",
        "                )\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported task: {task}\")\n",
        "            \n",
        "            return {\n",
        "                'project_name': project_name,\n",
        "                'relation_name': relation_name,\n",
        "                'view_table': view_table_query.strip(),\n",
        "                'query': query.strip(),\n",
        "                'preprocess': preprocess,\n",
        "            }\n",
        "\n",
        "        # === INFERENCE ===\n",
        "        else:\n",
        "            # Inference on the original table with filters but selection should be same as training\n",
        "            relation_name = f\"{self.pgml_schema}.{table_name}\"\n",
        "            \n",
        "            # Parse conditions\n",
        "            inference_conds_raw = sorted(intent.get(\"inference_condition\", []))\n",
        "            update_conds_raw = sorted(intent.get(\"update_condition\", []))\n",
        "            filter_like_updates, true_updates = self.split_update_conditions(update_conds_raw)\n",
        "            where_clause = []\n",
        "            for c in filter_like_updates:\n",
        "                col_u, op_u, val_u = self.parse_cond(c)\n",
        "                for i, ci in enumerate(inference_conds_raw):\n",
        "                    col_i, op_i, val_i = self.parse_cond(ci)\n",
        "\n",
        "                    # check if both conditions are on the same column and operation but different values(take the update one)\n",
        "                    if col_i == col_u and op_i == op_u and val_i != val_u:\n",
        "                        where_clause.append(f\"{self.quote_ident(col_u)} {op_u} {self.format_val(val_u)}\")\n",
        "                        inference_conds_raw.pop(i)\n",
        "\n",
        "            # append the rest of the inference conds\n",
        "            for c in inference_conds_raw:\n",
        "                col_i, op_i, val_i = self.parse_cond(c)\n",
        "                where_clause.append(f\"{self.quote_ident(col_i)} {op_i} {self.format_val(val_i)}\")\n",
        "            where_clause = \" AND \".join(where_clause)\n",
        "\n",
        "            if true_updates:\n",
        "                query = self.build_pgml_predict_sql_scenario2(\n",
        "                    project_name=project_name,\n",
        "                    relation_name=relation_name,\n",
        "                    input_feature_cols=input_feature_cols if input_feature_cols else [],\n",
        "                    where_clause=where_clause,\n",
        "                    true_updates=true_updates,\n",
        "                    limit=10\n",
        "                )\n",
        "            else:\n",
        "                query = self.build_pgml_predict_sql_scenario1(\n",
        "                    project_name=project_name,\n",
        "                    relation_name=relation_name,\n",
        "                    input_feature_cols=input_feature_cols,\n",
        "                    where_clause=where_clause,\n",
        "                    limit=10\n",
        "                )\n",
        "                \n",
        "            return {\n",
        "                'query': query.strip(),\n",
        "            }\n",
        "\n",
        "    def build_pgml_train_sql(self, project_name: str, task: str, relation_name: str, y_column_name: str, algorithm: str, \n",
        "                             preprocess: dict | None = None, test_size: float | None = None):\n",
        "        \"\"\"\n",
        "        preprocess: \n",
        "            impute: `error`, `mean`, `median`, `mode`, `min`, `max`, `zero`\n",
        "            scale: `preserve`, `standard`, `min_max`, `max_abs`, `robust`\n",
        "            encode: `native`, `target`, `one_hot`, `ordinal`\n",
        "        \"\"\"\n",
        "        args = [\n",
        "            f\"project_name => '{project_name}'\",\n",
        "            f\"task => '{task}'\",\n",
        "            f\"relation_name => '{relation_name}'\",\n",
        "            f\"algorithm => '{algorithm}'\",\n",
        "        ]\n",
        "        if task != 'clustering':\n",
        "            args.append(f\"y_column_name => '{y_column_name}'\")\n",
        "\n",
        "        if preprocess:\n",
        "            args.append(f\"preprocess => '{json.dumps(preprocess, ensure_ascii=False)}'\")\n",
        "        if test_size is not None:\n",
        "            args.append(f\"test_size => {test_size}\")\n",
        "        return \"SELECT pgml.train(\\n    \" + \",\\n    \".join(args) + \"\\n);\\n\"\n",
        "\n",
        "    def build_pgml_predict_sql_scenario1(self, project_name: str, relation_name: str, input_feature_cols: list[str], \n",
        "                                         where_clause: str = \"\", limit: int | None = None):\n",
        "        \"\"\"\n",
        "        Predict referencing columns directly (simple; no overrides).\n",
        "        \"\"\"\n",
        "        tuple_expr = \", \".join(self.quote_ident(c) for c in input_feature_cols)\n",
        "        sql = (\n",
        "            \"SELECT pgml.predict(\\n\"\n",
        "            f\"    '{project_name}', ({tuple_expr})\\n\"\n",
        "            f\") AS prediction\\n\"\n",
        "            f\"FROM {relation_name}\"\n",
        "        )\n",
        "        if where_clause:\n",
        "            sql += f\"\\nWHERE {where_clause}\"\n",
        "        if limit:\n",
        "            sql += f\"\\nLIMIT {limit}\"\n",
        "        sql += \";\\n\"\n",
        "        return sql\n",
        "\n",
        "    def build_pgml_predict_sql_scenario2(self, project_name: str, relation_name: str, input_feature_cols: list[str], \n",
        "                                         where_clause: str = \"\", true_updates: list[str] | None = None, limit: int | None = None) -> str:\n",
        "        select_items = []\n",
        "        true_updates = true_updates or []\n",
        "        override_map = {self.parse_cond(c)[0]: self.parse_cond(c)[2] for c in true_updates}\n",
        "        for col in input_feature_cols:\n",
        "            if col in override_map:\n",
        "                select_items.append(f\"{self.format_val(override_map[col])} AS {self.quote_ident(col)}\")\n",
        "            else:\n",
        "                select_items.append(f\"{self.quote_ident(col)}\")\n",
        "\n",
        "        inner_sql = \"SELECT \" + \", \".join(select_items) + f\" FROM {relation_name}\"\n",
        "        if where_clause:\n",
        "            inner_sql += f\" WHERE {where_clause}\"\n",
        "        if limit:\n",
        "            inner_sql += f\" LIMIT {limit}\"\n",
        "\n",
        "        return (\n",
        "            \"SELECT pgml.predict(\\n\"\n",
        "            f\"    '{project_name}', t\\n\"\n",
        "            \")\\n\"\n",
        "            \"FROM (\\n\"\n",
        "            f\"    {inner_sql}\\n\"\n",
        "            \") AS t;\\n\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total unique intent: 225 | S1: 131 | S2: 94\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import json\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "counter = defaultdict(set)\n",
        "with open('./data/train_final.jsonl', 'r') as file:\n",
        "    data = [json.loads(line) for line in file]\n",
        "\n",
        "for d in data:\n",
        "    counter['schema'].add(json.dumps(d['schema'], indent=2))\n",
        "    if d['intent'].get('update_condition'):\n",
        "        counter['s2'].add(json.dumps(d['intent'], indent=2))\n",
        "    else:\n",
        "        counter['s1'].add(json.dumps(d['intent'], indent=2))\n",
        "\n",
        "    counter['intent'].add(json.dumps(d['intent'], indent=2))\n",
        "    \n",
        "with open('./archieved/data/train_merged_CoT_final.jsonl', 'r') as f:\n",
        "    cots = [json.loads(line.strip()) for line in f.readlines()]\n",
        "\n",
        "print(f\"Total unique intent: {len(counter['intent'])} | S1: {len(counter['s1'])} | S2: {len(counter['s2'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2721/2721 [00:00<00:00, 253424.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total dataset entries: 837\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "gen_class = PostgresTemplateGenerator\n",
        "TABLE_DATASET_MAP = {v: k for k, v in DATASET_TABLE_MAP.items()}\n",
        "dataset = []\n",
        "checked_intents = set()\n",
        "for d in tqdm(data, total=len(data)):\n",
        "    k = json.dumps(d['intent'])\n",
        "    if k in checked_intents:\n",
        "        continue\n",
        "    schema = d['schema']['tables']\n",
        "    assert len(list(schema.keys())) == 1, \"len table is not 1\"\n",
        "    table_name = list(schema.keys())[0]\n",
        "    dataset_name = TABLE_DATASET_MAP[table_name]\n",
        "    intent = d['intent']\n",
        "\n",
        "    for model_name in gen_class.model_families.get(intent.get(\"task\"), []):\n",
        "        dataset.append({\n",
        "            'dataset_name': dataset_name,\n",
        "            'table_name': table_name,\n",
        "            'schema': schema,\n",
        "            'intent': intent,\n",
        "            'model_name': model_name\n",
        "    })\n",
        "    checked_intents.add(k)\n",
        "\n",
        "print(f\"Total dataset entries: {len(dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/837 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 837/837 [00:00<00:00, 19299.69it/s]\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "table_specific_exclusions = {\n",
        "    \"orders\": [\"user_id\", \"order_id\"],\n",
        "    \"disclosures_13\": [\"record_id\", \"family_id\", \"blanket_scope\", \"disclosure_event\", \"pub_cleaned\", \"wg_name\"],\n",
        "    \"outpatient_charges_2014\": [\"provider_id\"],\n",
        "    \"fips\": [\"GeoName\", \"GeoFIPS\"],\n",
        "    \"mbb_historical_teams_games\": [\"team_id\", \"name\", \"market\", \"opp_id\", \"opp_name\", \"opp_code\", \"opp_market\"],\n",
        "    \"historical_runups\": [\"id\", \"tsevent_id\", \"location_name\"],\n",
        "    \"solar_potential_by_postal_code\": [\"center_point\", \"install_size_kw_buckets\"],\n",
        "}\n",
        "\n",
        "outputs = []\n",
        "generator = gen_class()\n",
        "for x in tqdm(dataset, total=len(dataset)):\n",
        "    train_output = generator.gen(\n",
        "        dataset_name=x['dataset_name'],\n",
        "        table_name=x['table_name'],\n",
        "        schema=x['schema'],\n",
        "        intent=x['intent'],\n",
        "        is_train=True,\n",
        "        model_name=x['model_name'],\n",
        "        exclude_cols=table_specific_exclusions.get(x['table_name'], [])\n",
        "    )\n",
        "    # inference_output = generator.gen(\n",
        "    #     dataset_name=x['dataset_name'],\n",
        "    #     table_name=x['table_name'],\n",
        "    #     schema=x['schema'],\n",
        "    #     intent=x['intent'],\n",
        "    #     is_train=False,\n",
        "    #     model_name=x['model_name'],\n",
        "    #     exclude_cols=table_specific_exclusions.get(x['table_name'], [])\n",
        "    # )\n",
        "\n",
        "    outputs.append(train_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for o in tqdm(outputs, total=len(outputs)):\n",
        "    # create view_table\n",
        "    execute_query(o['view_table'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# create view_table\n",
        "execute_query(train_output['view_table'])\n",
        "\n",
        "time.sleep(1)\n",
        "\n",
        "# train model\n",
        "execute_query(train_output['query'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SELECT pgml.train(\n",
            "    project_name => 'sunroof_solar_solar_potential_by_postal_code_xgboost',\n",
            "    task => 'regression',\n",
            "    relation_name => 'public.sunroof_solar_solar_potential_by_postal_code_xgboost_view',\n",
            "    algorithm => 'xgboost',\n",
            "    y_column_name => 'number_of_panels_w',\n",
            "    preprocess => '{\"region_name\": {\"impute\": \"mode\", \"encode\": \"native\"}, \"state_name\": {\"impute\": \"mode\", \"encode\": \"native\"}, \"lat_max\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"lat_min\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"lng_max\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"lng_min\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"lat_avg\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"lng_avg\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"yearly_sunlight_kwh_kw_threshold_avg\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"count_qualified\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"percent_covered\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"percent_qualified\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"number_of_panels_n\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"number_of_panels_s\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"number_of_panels_e\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"number_of_panels_w\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"number_of_panels_f\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"number_of_panels_median\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"number_of_panels_total\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"kw_median\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"kw_total\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"yearly_sunlight_kwh_n\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"yearly_sunlight_kwh_s\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"yearly_sunlight_kwh_e\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"yearly_sunlight_kwh_w\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"yearly_sunlight_kwh_f\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"yearly_sunlight_kwh_median\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"yearly_sunlight_kwh_total\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"install_size_kw_buckets\": {\"impute\": \"mode\", \"encode\": \"native\"}, \"carbon_offset_metric_tons\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"existing_installs_count\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"center_point\": {\"impute\": \"mode\", \"encode\": \"native\"}}',\n",
            "    test_size => 0.1\n",
            ");\n"
          ]
        }
      ],
      "source": [
        "print(train_output['query'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"sunroof_solar/solar_potential_by_postal_code/clustering/kmeans\"\n",
            "public.\"sunroof_solar/solar_potential_by_postal_code/clustering/kmeans\"_view\n"
          ]
        }
      ],
      "source": [
        "print(train_output['project_name'])\n",
        "print(train_output['relation_name'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CREATE OR REPLACE VIEW public.sunroof_solar_solar_potential_by_postal_code_xgboost_view AS (\n",
            "SELECT \"region_name\", \"state_name\", \"lat_max\", \"lat_min\", \"lng_max\", \"lng_min\", \"lat_avg\", \"lng_avg\", \"yearly_sunlight_kwh_kw_threshold_avg\", \"count_qualified\", \"percent_covered\", \"percent_qualified\", \"number_of_panels_n\", \"number_of_panels_s\", \"number_of_panels_e\", \"number_of_panels_f\", \"number_of_panels_median\", \"number_of_panels_total\", \"kw_median\", \"kw_total\", \"yearly_sunlight_kwh_n\", \"yearly_sunlight_kwh_s\", \"yearly_sunlight_kwh_e\", \"yearly_sunlight_kwh_w\", \"yearly_sunlight_kwh_f\", \"yearly_sunlight_kwh_median\", \"yearly_sunlight_kwh_total\", \"carbon_offset_metric_tons\", \"existing_installs_count\", \"number_of_panels_w\" \n",
            "FROM public.solar_potential_by_postal_code);\n"
          ]
        }
      ],
      "source": [
        "print(train_output['view_table'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SELECT pgml.train(\n",
            "    project_name => 'sunroof_solar_solar_potential_by_postal_code_xgboost',\n",
            "    task => 'regression',\n",
            "    relation_name => 'public.sunroof_solar_solar_potential_by_postal_code_xgboost_view',\n",
            "    algorithm => 'xgboost',\n",
            "    y_column_name => 'number_of_panels_w',\n",
            "    preprocess => '{\"region_name\": {\"impute\": \"mode\", \"encode\": \"native\"}, \"state_name\": {\"impute\": \"mode\", \"encode\": \"native\"}, \"lat_max\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"lat_min\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"lng_max\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"lng_min\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"lat_avg\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"lng_avg\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"yearly_sunlight_kwh_kw_threshold_avg\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"count_qualified\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"percent_covered\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"percent_qualified\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"number_of_panels_n\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"number_of_panels_s\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"number_of_panels_e\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"number_of_panels_w\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"number_of_panels_f\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"number_of_panels_median\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"number_of_panels_total\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"kw_median\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"kw_total\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"yearly_sunlight_kwh_n\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"yearly_sunlight_kwh_s\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"yearly_sunlight_kwh_e\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"yearly_sunlight_kwh_w\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"yearly_sunlight_kwh_f\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"yearly_sunlight_kwh_median\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"yearly_sunlight_kwh_total\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"install_size_kw_buckets\": {\"impute\": \"mode\", \"encode\": \"native\"}, \"carbon_offset_metric_tons\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"existing_installs_count\": {\"impute\": \"median\", \"scale\": \"standard\"}, \"center_point\": {\"impute\": \"mode\", \"encode\": \"native\"}}',\n",
            "    test_size => 0.1\n",
            ");\n"
          ]
        }
      ],
      "source": [
        "print(train_output['query'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SELECT pgml.predict(\n",
            "    'sunroof_solar_solar_potential_by_postal_code_xgboost', (\"region_name\", \"state_name\", \"lat_max\", \"lat_min\", \"lng_max\", \"lng_min\", \"lat_avg\", \"lng_avg\", \"yearly_sunlight_kwh_kw_threshold_avg\", \"count_qualified\", \"percent_covered\", \"percent_qualified\", \"number_of_panels_n\", \"number_of_panels_s\", \"number_of_panels_e\", \"number_of_panels_f\", \"number_of_panels_median\", \"number_of_panels_total\", \"kw_median\", \"kw_total\", \"yearly_sunlight_kwh_n\", \"yearly_sunlight_kwh_s\", \"yearly_sunlight_kwh_e\", \"yearly_sunlight_kwh_w\", \"yearly_sunlight_kwh_f\", \"yearly_sunlight_kwh_median\", \"yearly_sunlight_kwh_total\", \"carbon_offset_metric_tons\", \"existing_installs_count\")\n",
            ") AS prediction\n",
            "FROM public.solar_potential_by_postal_code\n",
            "WHERE \"number_of_panels_n\" = 1386\n",
            "LIMIT 10;\n"
          ]
        }
      ],
      "source": [
        "print(inference_output[\"query\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_output' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# models.id, models.algorithm, models.metrics\u001b[39;00m\n\u001b[32m      2\u001b[39m metric_query = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mSELECT * \u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33mFROM pgml.models AS m \u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33mJOIN pgml.projects AS p \u001b[39m\n\u001b[32m      5\u001b[39m \u001b[33m ON p.id = m.project_id\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[33mWHERE p.name = \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtrain_output\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mproject_name\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      8\u001b[39m execute_query(metric_query, fetch=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[31mNameError\u001b[39m: name 'train_output' is not defined"
          ]
        }
      ],
      "source": [
        "# models.id, models.algorithm, models.metrics\n",
        "metric_query = f\"\"\"SELECT * \n",
        "FROM pgml.models AS m \n",
        "JOIN pgml.projects AS p \n",
        " ON p.id = m.project_id\n",
        "WHERE p.name = '{train_output['project_name']}';\"\"\"\n",
        "\n",
        "execute_query(metric_query, fetch=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# drop records in the table in pgml.models\n",
        "drop_query = f\"\"\"\n",
        "DELETE FROM pgml.models\n",
        "WHERE project_id IN (15, 280)\n",
        "\"\"\"\n",
        "# WHERE project_id = (SELECT id FROM pgml.projects WHERE name = '{train_output['project_name']}');\n",
        "\n",
        "execute_query(drop_query, fetch=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# drop all projects in the table in pgml.projects\n",
        "drop_query = f\"\"\"\n",
        "DELETE FROM pgml.projects\n",
        "WHERE id IN (14, 15, 280)\n",
        "\"\"\"\n",
        "# WHERE project_id = (SELECT id FROM pgml.projects WHERE name = '{train_output['project_name']}');\n",
        "\n",
        "execute_query(drop_query, fetch=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nSELECT *\\nFROM pgml.models\\nJOIN pgml.projects\\n ON projects.id = models.project_id\\nWHERE projects.project_name = 'sunroof_solar_solar_potential_by_postal_code_kmeans';\\n\""
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metric_query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxT8Blho4R_m",
        "outputId": "cf8c131e-772e-446c-d609-aceb76a6fd84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Generated SQL ===\n",
            "SELECT pgml.train(\n",
            "    project_name => 'sample_dataset_sales_data_regression_ce2f02bf7984',\n",
            "    task => 'regression',\n",
            "    relation_name => 'public.\"sales_data\"',\n",
            "    y_column_name => 'revenue',\n",
            "    algorithm => 'xgboost'\n",
            ");\n",
            "\n"
          ]
        }
      ],
      "source": [
        "intent = {\n",
        "    \"dataset_name\": \"sample_dataset\",\n",
        "    \"table_id\": \"sales_data\",\n",
        "    \"output\": {\n",
        "        \"task\": \"regression\",\n",
        "        \"time_series\": \"True\",\n",
        "        \"target_column\": \"<col>revenue</col>\",\n",
        "        \"inference_condition\": [\n",
        "            \"<col>region</col><op>=</op><val>west</val>\"\n",
        "        ],\n",
        "        \"update_condition\": [\n",
        "            \"<col>revenue</col><op>></op><val>1000</val>\"\n",
        "        ]\n",
        "    },\n",
        "    \"data_dictionary\": {\n",
        "        \"revenue\": {\"type\": \"FLOAT\"},\n",
        "        \"date\": {\"type\": \"DATE\"},\n",
        "        \"region\": {\"type\": \"STRING\"},\n",
        "        \"lat_long\": {\"type\": \"GEOGRAPHY\"}\n",
        "    }\n",
        "}\n",
        "\n",
        "# Parameters for generation\n",
        "platform_type = \"PostgreML\"  # or \"PostgreML\"\n",
        "todo_type = \"train\"         # or \"predict\"\n",
        "model_name = \"my_arima_model\"\n",
        "\n",
        "# Run test\n",
        "template = Template()\n",
        "try:\n",
        "    sql_output = template.gen(platform_type, intent, todo_type, model_name)\n",
        "    print(\"=== Generated SQL ===\")\n",
        "    print(sql_output)\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nl2ml-sql (3.11.13)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
