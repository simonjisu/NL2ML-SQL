{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4626ea32-9b70-4df5-a3c4-add534f5a689",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 05-20 03:52:31 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 05-20 03:52:31 [__init__.py:239] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.5.3: Fast Llama patching. Transformers: 4.51.3. vLLM: 0.8.5.post1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.684 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.5.3 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
       "        (layers): ModuleList(\n",
       "          (0): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "          (1): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "          (2-31): 30 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# === 모델 설정 ===\n",
    "max_seq_length = 5500\n",
    "load_in_4bit = True\n",
    "dtype = None\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name     = \"unsloth/Llama-3.1-8B-unsloth-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit   = load_in_4bit,\n",
    "    dtype          = dtype,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "adapter_path = \"/shared/s1/lab11/BigqueryML/SFT_fine_tuned_llama_3.1_8B\"\n",
    "model.load_adapter(adapter_path, adapter_name=\"default\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "160a7371-f51a-41f3-ae60-d7661911aa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === I/O 설정 ===\n",
    "input_path  = \"/shared/s1/lab11/BigqueryML/test_set_small.jsonl\"\n",
    "output_path = \"/shared/s1/lab11/BigqueryML/test_set_small_inference_self_consistency(16).jsonl\"\n",
    "\n",
    "# === Prompt 생성 ===\n",
    "def make_prompt(instruction, input_table):\n",
    "    return f\"\"\"\n",
    "We aim to extract structured machine learning configuration arguments and conditions from natural language question and a given data dictionary.\n",
    "These arguments are essential to automatically generate BigQuery ML SQL code.\n",
    "The output must strictly follow the specified format and use the keys as described below.\n",
    "\n",
    "### Output Format:\n",
    "The output should be a JSON object containing the following keys:\n",
    "\n",
    "1. **time_series** (boolean): Indicates whether the model is intended for time series forecasting.\n",
    "   - Example: \"time_series\": \"False\"\n",
    "   - Use \"True\" if Input involves time columns such as \"Date\", otherwise \"False\".\n",
    "\n",
    "2. **target_column** (string): The column name that represents the target variable to predict.\n",
    "   - Use the format: \"<col>column_name</col>\"\n",
    "   - Example: \"target_column\": \"<col>clarity</col>\"\n",
    "\n",
    "3. **inference_condition** (list of strings): A list of conditions used for inference or prediction. Each condition should specify a column, an operator, and a value.\n",
    "   - Use the format: \"<col>column_name</col><op>operator</op><val>value</val>\"\n",
    "   - Multiple conditions can be provided as a list.\n",
    "   - Example: \"inference_condition\": [\"<col>carat</col><op>>=</op><val>1.0</val>\", \"<col>color</col><op>=</op><val>J</val>\"]\n",
    "\n",
    "4. **update_condition** (list of strings, optional): A list of conditions for updating the data or model. Similar to `inference_condition`, it specifies column, operator, and value.\n",
    "   - Example: \"update_condition\": [\"<col>color</col><op>=</op><val>G</val>\"]\n",
    "   - - If there is no change in the conditions as per the instruction, this key should not be generated.\n",
    "\n",
    "5. **task** (string): The type of machine learning task to perform.\n",
    "   - Common values: \"classification\", \"regression\", \"clustering\", \"anomaly_detection\"\n",
    "   - Example: \"task\": \"classification\"\n",
    "\n",
    "### Natural Language Question:\n",
    "{instruction}\n",
    "\n",
    "### Data Dictionary:\n",
    "{input_table}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98ba6b26-7480-47be-bf2f-c370a3b6869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Self-Consistency Inference 함수 ===\n",
    "from collections import Counter\n",
    "\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "PAT = re.compile(\n",
    "    r\"^\\s*###\\s*Output:?\\s*[\\r\\n]+(.*?)(?=^\\s*###|\\Z)\",\n",
    "    flags=re.I | re.S | re.M,\n",
    ")\n",
    "\n",
    "def extract_output_block(pred_str: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    예측 문자열(pred_str)에서 '### Output:' 블록을 찾아 반환.\n",
    "    없으면 None.\n",
    "    \"\"\"\n",
    "    m = PAT.search(pred_str)\n",
    "    return m.group(1).strip() if m else None\n",
    "\n",
    "def infer_self_consistency(\n",
    "    prompt: str,\n",
    "    num_samples: int = 2,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.8,\n",
    "    max_new_tokens: int = 2700\n",
    "):\n",
    "    # 1) 프롬프트 토큰화\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # 2) 샘플 생성\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            num_return_sequences=num_samples,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=False,\n",
    "        )\n",
    "    seqs = outputs.sequences  # (num_samples, seq_len)\n",
    "\n",
    "    # 3) 디코딩 + '### Output:' 블록 추출\n",
    "    samples = []\n",
    "    for seq in seqs:\n",
    "        decoded = tokenizer.decode(seq, skip_special_tokens=True)\n",
    "        cleaned  = extract_output_block(decoded)     # ⬅️ 변경된 부분\n",
    "        # '### Output:' 블록이 없으면 전체 문자열을 사용\n",
    "        samples.append(cleaned if cleaned is not None else decoded.strip())\n",
    "\n",
    "    # 4) 최다 득표(best) 선택\n",
    "    best_output, _ = Counter(samples).most_common(1)[0]\n",
    "\n",
    "    # 5) 전체 토큰 합계 계산 (prompt + generated 모두)\n",
    "    total_full_tokens = sum(seq.shape[-1] for seq in seqs)\n",
    "\n",
    "    return best_output, total_full_tokens, samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5127362c-b624-457e-a449-dffccf899b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [2:50:16, 51.08s/it]\n"
     ]
    }
   ],
   "source": [
    "# === JSONL 처리 ===\n",
    "with open(input_path, \"r\") as infile, open(output_path, \"w\") as outfile:\n",
    "    for line in tqdm(infile):\n",
    "        ex = json.loads(line)\n",
    "        instruction = ex[\"instruction\"]\n",
    "        input_table = ex[\"input\"]\n",
    "        ground_truth = ex.get(\"output\")\n",
    "\n",
    "        prompt = make_prompt(instruction, input_table)\n",
    "        try:\n",
    "            best, total_tokens, all_samples = infer_self_consistency(\n",
    "                prompt,\n",
    "                num_samples=16,      # 원하시는 만큼 샘플 수 조정\n",
    "                temperature=0.8,\n",
    "                top_p=0.9,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            best, total_tokens, all_samples = f\"ERROR: {e}\", 0, []\n",
    "\n",
    "        result = {\n",
    "            \"instruction\": instruction,\n",
    "            \"input\": input_table,\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"selfconsistency_best\": best,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"all_samples\": all_samples\n",
    "        }\n",
    "\n",
    "        outfile.write(json.dumps(result, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429605da-3521-466a-a344-7dc82bf50a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVAL\n",
    "\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "class RewardCalculator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        weights = None,\n",
    "        use_fuzzy_matching = True,\n",
    "        normalize = True,  # True means `filter` mode other than `grpo` mode\n",
    "        verbose = False\n",
    "    ):\n",
    "        self.use_fuzzy_matching = use_fuzzy_matching\n",
    "        self.verbose = verbose\n",
    "        self.normalize = normalize\n",
    "        self.fail_val = 0.0 if normalize else -1.0\n",
    "\n",
    "        # Fixed weights\n",
    "        self.weights = weights or {\n",
    "            \"time_series\": 0.1,\n",
    "            \"target_column\": 0.5,\n",
    "            \"inference_condition\": 0.3,\n",
    "            \"update_condition\": 0.4,\n",
    "            \"task\": 0.7\n",
    "        }\n",
    "\n",
    "        # Required keys\n",
    "        self.required_keys = [\"time_series\", \"target_column\", \"inference_condition\", \"task\"]\n",
    "\n",
    "\n",
    "    def _match(self, a, b, key=None):\n",
    "\n",
    "        def jaccard(set1, set2):\n",
    "            intersection = len(set1 & set2)\n",
    "            union = len(set1 | set2)\n",
    "            if union != 0:\n",
    "                # Jaccard similarity\n",
    "                result = intersection / union\n",
    "                if result == 0.0:\n",
    "                    return self.fail_val\n",
    "                return result\n",
    "            else:\n",
    "                return self.fail_val\n",
    "\n",
    "        def extract_condition_parts(condition_str):\n",
    "            condition_str = condition_str.strip()\n",
    "\n",
    "            col = re.findall(r\"<col>(.*?)</col>\", condition_str)\n",
    "            op = re.findall(r\"<op>(.*?)</op>\", condition_str)\n",
    "            val = re.findall(r\"<val>(.*?)</val>\", condition_str)\n",
    "\n",
    "            col_val = col[0].strip() if col else \"\"\n",
    "            op_val = op[0].strip() if op else \"\"\n",
    "            val_val = val[0].strip() if val else \"\"\n",
    "\n",
    "            # Step 2: Heuristics if any of the parts are missing\n",
    "            if not (col_val and op_val and val_val):\n",
    "                # Remove tags to simplify raw parsing\n",
    "                clean_str = re.sub(r\"</?[^>]+>\", \"\", condition_str)\n",
    "\n",
    "                # Try simple expression pattern: col op val\n",
    "                match = re.match(r\"([a-zA-Z0-9_.]+)\\s*([=!<>]+)\\s*(.+)\", clean_str)\n",
    "                if match:\n",
    "                    if not col_val:\n",
    "                        col_val = match.group(1).strip()\n",
    "                    if not op_val:\n",
    "                        op_val = match.group(2).strip()\n",
    "                    if not val_val:\n",
    "                        val_val = match.group(3).strip()\n",
    "\n",
    "            return col_val, op_val, val_val\n",
    "\n",
    "        def tag_completeness_score(cond_str, tags):\n",
    "            present = sum(tag in cond_str for tag in tags)\n",
    "            return present / len(tags)\n",
    "\n",
    "        def score_pair(a_cond, b_cond):\n",
    "            a_cond = a_cond.strip().lower()\n",
    "            b_cond = b_cond.strip().lower()\n",
    "            a_col, a_op, a_val = extract_condition_parts(a_cond)\n",
    "            b_col, b_op, b_val = extract_condition_parts(b_cond)\n",
    "\n",
    "            col_score = jaccard({a_col}, {b_col})\n",
    "            op_score = jaccard({a_op}, {b_op})\n",
    "            val_score = int(SequenceMatcher(None, a_val, b_val).ratio() >= 0.9)\n",
    "\n",
    "            if col_score == 0 or op_score == 0 or val_score == 0:\n",
    "                return self.fail_val\n",
    "\n",
    "            avg_score = (col_score + op_score + val_score) / 3\n",
    "            tag_score = tag_completeness_score(a_cond, [\"<col>\", \"</col>\", \"<op>\", \"</op>\", \"<val>\", \"</val>\"])\n",
    "            if tag_score != 1.0:\n",
    "                return self.fail_val\n",
    "\n",
    "            return avg_score\n",
    "\n",
    "        if key in {\"inference_condition\", \"update_condition\"}:\n",
    "            a_list = a if isinstance(a, list) else [a]\n",
    "            b_list = b if isinstance(b, list) else [b]\n",
    "\n",
    "            if not a_list and not b_list:\n",
    "                return 1.0\n",
    "            if not a_list or not b_list:\n",
    "                return self.fail_val\n",
    "\n",
    "            used_b_indices = set()\n",
    "            matched_scores = []\n",
    "\n",
    "            for a_cond in a_list:\n",
    "                best_score = self.fail_val\n",
    "                best_j = None\n",
    "                for j, b_cond in enumerate(b_list):\n",
    "                    if j in used_b_indices:\n",
    "                        continue\n",
    "                    score = score_pair(a_cond, b_cond)\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_j = j\n",
    "                if best_j is not None:\n",
    "                    used_b_indices.add(best_j)\n",
    "                matched_scores.append(best_score)  # score is 0.0 if unmatched\n",
    "\n",
    "            # Final score is average over max(len(predicted), len(ground_truth))\n",
    "            final_score = sum(matched_scores) / max(len(a_list), len(b_list))\n",
    "            return final_score\n",
    "\n",
    "        # Default Jaccard (for all non-condition fields)\n",
    "        a_str = \" \".join(map(str, a)) if isinstance(a, list) else str(a)\n",
    "        b_str = \" \".join(map(str, b)) if isinstance(b, list) else str(b)\n",
    "\n",
    "        if not a_str.strip() and not b_str.strip():\n",
    "            return 1.0\n",
    "        if not a_str.strip() or not b_str.strip():\n",
    "            return self.fail_val\n",
    "\n",
    "        if key == \"target_column\":\n",
    "\n",
    "            def strip_tags(text):\n",
    "                return re.sub(r\"</?[^>]+>\", \"\", text).strip().lower()\n",
    "\n",
    "            a_clean = strip_tags(a_str)\n",
    "            b_clean = strip_tags(b_str)\n",
    "\n",
    "            tag_score = tag_completeness_score(a_str, [\"<col>\", \"</col>\"])\n",
    "            if tag_score != 1.0:\n",
    "                return self.fail_val\n",
    "\n",
    "            sim_score = jaccard({a_clean}, {b_clean})\n",
    "            return sim_score\n",
    "\n",
    "        a_tokens = set(a_str.lower().split())\n",
    "        b_tokens = set(b_str.lower().split())\n",
    "\n",
    "        return jaccard(a_tokens, b_tokens)\n",
    "\n",
    "\n",
    "    def weighted_accuracy(self, predicted, ground_truth):\n",
    "        \"\"\"Computes weighted accuracy + diagnostics\"\"\"\n",
    "        matches = {}\n",
    "        diagnostics = {}\n",
    "\n",
    "        weights = self.weights.copy()\n",
    "\n",
    "        # Required keys\n",
    "        for key in self.required_keys:\n",
    "            sim_score = self._match(predicted.get(key, []), ground_truth.get(key, []), key=key)\n",
    "            matches[key] = sim_score\n",
    "            diagnostics[key] = weights.get(key, 0) * sim_score\n",
    "\n",
    "        # Optional key: update_condition\n",
    "        has_update_condition = \"update_condition\" in ground_truth and (ground_truth.get(\"update_condition\") not in (None, []))\n",
    "        if has_update_condition:\n",
    "            if predicted.get(\"update_condition\") is None:\n",
    "                predicted['update_condition'] = []\n",
    "            sim_score = self._match(\n",
    "                predicted.get(\"update_condition\", []),\n",
    "                ground_truth.get(\"update_condition\", []),\n",
    "                key=\"update_condition\"\n",
    "            )\n",
    "            matches[\"update_condition\"] = sim_score\n",
    "            diagnostics[\"update_condition\"] = weights.get(\"update_condition\") * sim_score\n",
    "\n",
    "        # Dynamically decide which keys to include in normalization\n",
    "        active_keys = self.required_keys.copy()\n",
    "        if has_update_condition:\n",
    "            active_keys.append(\"update_condition\")\n",
    "\n",
    "        if self.normalize:\n",
    "            # filter mode: normalize the score\n",
    "            max_possible_score = sum(weights.get(k, 0) for k in active_keys)\n",
    "            weighted_score = sum(diagnostics.values())\n",
    "            final_score = max(0.0, min(1.0, weighted_score / max_possible_score))\n",
    "        else:\n",
    "            # grpo mode: No normalization\n",
    "            final_score = sum(diagnostics.values())\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"[Reward Diagnostics]\")\n",
    "            print(\"Matches:\", matches)\n",
    "            print(\"Diagnostics (per-key contribution):\", diagnostics)\n",
    "            print(\"Weighted Score:\", final_score)\n",
    "\n",
    "        return round(final_score, 6), matches, diagnostics\n",
    "\n",
    "    def get_min_possible_reward(self, ground_truth, convert_to_max=False):\n",
    "        w = sum([self.weights.get(k) for k in ground_truth.keys() if self.weights.get(k) is not None])\n",
    "        r = round(self.fail_val * w, 6)\n",
    "        return -r if convert_to_max else r\n",
    "\n",
    "    def self_check(self, intermediate_output):\n",
    "        \"\"\"Checks for presence of required keys.\"\"\"\n",
    "        for key in self.required_keys:\n",
    "            if key not in intermediate_output:\n",
    "                return {\"status\": \"INVALID\", \"reason\": f\"Missing required key: {key}\"}\n",
    "        return {\"status\": \"VALID\", \"intermediate_output\": intermediate_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01ae4f3-cadb-4f7e-90ee-c6cdcca7958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator/evaluator.py\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, mode: str = \"filter\", threshold: float = 0.9, verbose: bool = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mode: 'filter' or 'grpo'\n",
    "            threshold: filter cutoff (used only in filter mode)\n",
    "        \"\"\"\n",
    "        assert mode in (\"filter\", \"grpo\")\n",
    "        self.mode = mode\n",
    "        self.threshold = threshold\n",
    "        self.calculator = RewardCalculator(verbose=verbose, normalize=True if mode == \"filter\" else False)\n",
    "\n",
    "    def evaluate(self, predicted: dict, ground_truth: dict) -> dict:\n",
    "        validity = self.calculator.self_check(predicted)\n",
    "        if validity[\"status\"] != \"VALID\":\n",
    "            result = {\"score\": 0.0, \"diagnostics\": {\"status\": validity[\"status\"], \"reason\": validity[\"reason\"]}}\n",
    "            if self.mode == \"filter\":\n",
    "                return {\"keep\": False, **result}\n",
    "            elif self.mode == \"grpo\":\n",
    "                return {\"reward\": self.calculator.get_min_possible_reward(ground_truth), **result}\n",
    "\n",
    "        score, matches, diagnostics = self.calculator.weighted_accuracy(predicted, ground_truth)\n",
    "\n",
    "        if self.mode == \"filter\":\n",
    "            return {\n",
    "                \"keep\": score >= self.threshold,\n",
    "                \"score\": score,\n",
    "                \"diagnostics\": {\n",
    "                    \"matches\": matches,\n",
    "                    \"contributions\": diagnostics,\n",
    "                    \"total\": score\n",
    "                }\n",
    "            }\n",
    "        elif self.mode == \"grpo\":\n",
    "            return {\n",
    "                \"reward\": score,\n",
    "                \"diagnostics\": {\n",
    "                    \"matches\": matches,\n",
    "                    \"contributions\": diagnostics,\n",
    "                    \"total\": score\n",
    "                }\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be2dc00-97e0-4995-876a-768d07faea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ast\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ─── 1) 파일 경로 설정 ─────────────────────────────────────────────\n",
    "INPUT_PATH  = Path(\"/shared/s1/lab11/BigqueryML/test_set_small_inference_self_consistency(16).jsonl\")\n",
    "OUTPUT_PATH = INPUT_PATH.with_name(INPUT_PATH.stem + \"_with_best_of_n.jsonl\")\n",
    "\n",
    "# ─── 2) 평가기 준비 (filter vs grpo 원하는 모드 선택) ──────────────\n",
    "evaluator = Evaluator(mode=\"filter\", verbose=False)   # 점수만 필요하면 grpo\n",
    "\n",
    "# ─── 3) 유틸: 문자열(dict 형태) → 파이썬 dict 변환 ───────────────\n",
    "def str_to_dict(sample_str: str) -> dict:\n",
    "    \"\"\"\n",
    "    all_samples 안에 들어 있는 단일/이중따옴표 혼합 문자열을\n",
    "    안전하게 dict로 변환. ast.literal_eval 사용.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return ast.literal_eval(sample_str)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"샘플 문자열 파싱 실패: {e}\\n{sample_str}\")\n",
    "\n",
    "# ─── 4) 메인 루프 ──────────────────────────────────────────────────\n",
    "with INPUT_PATH.open(encoding=\"utf-8\") as fin, \\\n",
    "     OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "\n",
    "    for line in tqdm(fin, desc=\"rerank\"):\n",
    "        record = json.loads(line)\n",
    "\n",
    "        ground_truth = record.get(\"ground_truth\", {})\n",
    "        samples_str  = record.get(\"all_samples\", [])\n",
    "\n",
    "        # 각 샘플 평가\n",
    "        scored = []\n",
    "        for s in samples_str:\n",
    "            pred_dict = str_to_dict(s)\n",
    "            result = evaluator.evaluate(pred_dict, ground_truth)\n",
    "            # grpo 모드면 result[\"reward\"], filter 모드면 result[\"score\"]\n",
    "            score_key = \"reward\" if evaluator.mode == \"grpo\" else \"score\"\n",
    "            score = result.get(score_key, 0.0)\n",
    "            scored.append((score, s))\n",
    "\n",
    "        # 최고 점수 샘플 선택 (tie 시 첫 번째)\n",
    "        scored.sort(key=lambda x: x[0], reverse=True)\n",
    "        best_sample_str = scored[0][1]\n",
    "        best_score      = scored[0][0]\n",
    "\n",
    "        # 새 필드 추가\n",
    "        record[\"best_of_n\"]      = best_sample_str\n",
    "        record[\"best_of_n_score\"] = best_score   # 디버깅용 (원치 않으면 삭제)\n",
    "\n",
    "        fout.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"✅ 저장 완료 → {OUTPUT_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
